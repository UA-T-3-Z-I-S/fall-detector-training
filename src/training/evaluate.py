import numpy as np
import os
from keras.models import load_model
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from collections import Counter

from src.dataset_loader.buffer_generator import BufferGenerator
from src.config.paths import BUFFER_PATHS

def evaluate():
    print("ğŸ“¦ Preparando test generator...")
    batch_size = 12

    test_gen = BufferGenerator(
        BUFFER_PATHS['test']['caida'],
        BUFFER_PATHS['test']['no_caida'],
        batch_size=batch_size,
        shuffle=False
    )

    base_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    models_dir = os.path.join(base_dir, "keras", "epoch_3")
    os.makedirs(models_dir, exist_ok=True)

    model_path = os.path.join(models_dir, "fall_detector_model.keras")

    if not os.path.exists(model_path):
        print(f"âŒ Modelo no encontrado en: {model_path}")
        return

    print(f"ğŸ“¥ Cargando modelo desde {model_path}...")
    model = load_model(model_path)

    print("ğŸ§ª Evaluando modelo en test...")
    y_true, y_pred = [], []

    for i in range(len(test_gen)):
        X_batch, y_batch, _ = test_gen[i]

        if X_batch is None or len(X_batch) == 0:
            print(f"âš ï¸ Batch {i + 1} vacÃ­o, saltando...")
            continue

        print(f"ğŸ”¢ Evaluando batch {i + 1}/{len(test_gen)}...")

        try:
            y_probs = model.predict(X_batch, verbose=0)
        except Exception as e:
            print(f"âŒ Error al predecir el batch {i + 1}: {e}")
            continue

        # âœ… Convertir probabilidad a clase binaria
        preds = (y_probs > 0.5).astype(int).flatten()

        y_true.extend(y_batch)
        y_pred.extend(preds)

    if not y_true:
        print("âŒ No se pudo evaluar: y_true vacÃ­o.")
        return

    # âœ… DiagnÃ³stico de distribuciÃ³n
    print("\nğŸ” DistribuciÃ³n de clases:")
    print("y_true :", Counter(y_true))
    print("y_pred :", Counter(y_pred))

    # âœ… MÃ©tricas
    acc = accuracy_score(y_true, y_pred)
    prec = precision_score(y_true, y_pred, zero_division=0)
    rec = recall_score(y_true, y_pred, zero_division=0)
    f1 = f1_score(y_true, y_pred, zero_division=0)

    print(f"\nğŸ“ˆ Resultados en test:")
    print(f"Accuracy:  {acc:.4f}")
    print(f"Precision: {prec:.4f}")
    print(f"Recall:    {rec:.4f}")
    print(f"F1 Score:  {f1:.4f}")

    # âœ… Guardar resultados
    results_path = os.path.join(models_dir, "results_test.txt")
    with open(results_path, "w") as f:
        f.write("Resultados del modelo:\n")
        f.write(f"Accuracy:  {acc:.4f}\n")
        f.write(f"Precision: {prec:.4f}\n")
        f.write(f"Recall:    {rec:.4f}\n")
        f.write(f"F1 Score:  {f1:.4f}\n")

    # âœ… Guardar modelo con nombre segÃºn mÃ©tricas
    model_name = os.path.join(models_dir, f"model_acc{int(acc*100)}_f1{int(f1*100)}.keras")
    model.save(model_name)

    print(f"\nğŸ’¾ Modelo guardado como: {model_name}")
    print(f"ğŸ“ Resultados guardados en: {results_path}")

if __name__ == "__main__":
    evaluate()
